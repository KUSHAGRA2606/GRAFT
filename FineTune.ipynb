{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file deals with fine tuning (supervised fine tuning) a given LLM model using the RAFT methodology(retrieval augmented fine tuning)"
      ],
      "metadata": {
        "id": "moSV0pFzk97a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Libraries"
      ],
      "metadata": {
        "id": "yO2N27wUCiu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers is a state of art library for working with models like GPT-2,BERT etc.Peft stands for parameter efficient fine tuning it allows you to fine tune only a small fraction of model's parameter.bitsandbytes provides powerful quantization techniques,Torch is a core pytorch library one of the most popular,PyTorch is the foundation upon which all the other libraries are built"
      ],
      "metadata": {
        "id": "tL_e8TefUT2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1VWOzE5BIo-"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Installations\n",
        "!pip install -q transformers peft bitsandbytes accelerate datasets torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the RAFT-like Dataset ,For a given dataset we have 80 percent of this consisting of distractor with golden document and the rest 20 percent consists of only distractor document.\n"
      ],
      "metadata": {
        "id": "ZymbXePVCt4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Data Generation and Inspection\n",
        "import json\n",
        "import random\n",
        "from datasets import Dataset\n",
        "\n",
        "def generate_raft_dataset(num_examples: int = 100, p_golden_fraction: float = 0.8):\n",
        "    \"\"\"\n",
        "    Generates a synthetic dataset mimicking the RAFT structure with golden and distractor documents.\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "\n",
        "    # Define some reusable components\n",
        "    questions = [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Who discovered penicillin?\",\n",
        "        \"When did World War II end?\",\n",
        "        \"What is the chemical symbol for water?\",\n",
        "        \"Which planet is known as the Red Planet?\"\n",
        "    ]\n",
        "\n",
        "    golden_docs_data = {\n",
        "        \"What is the capital of France?\": {\n",
        "            \"text\": \"Paris is the capital and most populous city of France.\",\n",
        "            \"answer\": \"Paris\",\n",
        "            \"reason\": \"The document explicitly states 'Paris is the capital... of France'.\"\n",
        "        },\n",
        "        \"Who discovered penicillin?\": {\n",
        "            \"text\": \"Alexander Fleming, a Scottish physician, discovered penicillin in 1928.\",\n",
        "            \"answer\": \"Alexander Fleming\",\n",
        "            \"reason\": \"The document clearly states 'Alexander Fleming... discovered penicillin'.\"\n",
        "        },\n",
        "        \"When did World War II end?\": {\n",
        "            \"text\": \"World War II officially ended with the formal surrender of Japan on September 2, 1945.\",\n",
        "            \"answer\": \"September 2, 1945\",\n",
        "            \"reason\": \"The document specifies 'World War II officially ended... on September 2, 1945'.\"\n",
        "        },\n",
        "        \"What is the chemical symbol for water?\": {\n",
        "            \"text\": \"The chemical symbol for water is Hâ‚‚O.\",\n",
        "            \"answer\": \"Hâ‚‚O\",\n",
        "            \"reason\": \"The document directly provides 'The chemical symbol for water is Hâ‚‚O'.\"\n",
        "        },\n",
        "        \"Which planet is known as the Red Planet?\": {\n",
        "            \"text\": \"Mars is often referred to as the Red Planet due to its reddish appearance.\",\n",
        "            \"answer\": \"Mars\",\n",
        "            \"reason\": \"The document states 'Mars is often referred to as the Red Planet'.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    distractor_docs_pool = [\n",
        "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris.\",\n",
        "        \"Antibiotics are medicines that fight infections caused by bacteria.\",\n",
        "        \"The Cold War was a period of geopolitical tension between the United States and the Soviet Union.\",\n",
        "        \"Oxygen is a chemical element with the symbol O and atomic number 8.\",\n",
        "        \"Jupiter is the largest planet in our solar system, known for its Great Red Spot.\"\n",
        "    ]\n",
        "\n",
        "    instruction = \"Instruction: Given the question and context, provide a logical reasoning and the final answer. Use the format: ##Reason: {reason}\\n##Answer: {answer}.\"\n",
        "   #This starts a loop that will run num_examples times\n",
        "   #At each iteration a random question is selected form golden_docs_Data\n",
        "    for i in range(num_examples):\n",
        "        q = random.choice(list(golden_docs_data.keys()))\n",
        "        golden_data = golden_docs_data[q]\n",
        "        golden_doc = golden_data[\"text\"]\n",
        "        correct_answer = golden_data[\"answer\"]\n",
        "        cot_reason = golden_data[\"reason\"]\n",
        "\n",
        "        current_distractors = random.sample(distractor_docs_pool, k=min(2, len(distractor_docs_pool)))\n",
        "        context_docs = current_distractors[:]\n",
        "\n",
        "        # Decide whether to include the golden document based on P\n",
        "        use_golden = random.random() < p_golden_fraction\n",
        "\n",
        "        if use_golden:\n",
        "            context_docs.append(golden_doc)\n",
        "            random.shuffle(context_docs)\n",
        "\n",
        "        context_str = \"\\n\".join(f\"[Document {j+1}: {doc}]\" for j, doc in enumerate(context_docs))\n",
        "\n",
        "        input_text = f\"Question: {q}\\nContext: {context_str}\\n{instruction}\"\n",
        "        output_text = f\"##Reason: {cot_reason}\\n##Answer: {correct_answer}\"\n",
        "\n",
        "        dataset.append({\"input\": input_text, \"output\": output_text})\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Generate the dataset\n",
        "print(\"Generating RAFT-like dataset...\")\n",
        "raft_training_data = generate_raft_dataset(num_examples=200, p_golden_fraction=0.8)\n",
        "print(f\"Generated {len(raft_training_data)} training examples.\")\n",
        "\n",
        "# Convert to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_list(raft_training_data)\n",
        "\n",
        "print(\"\\n--- Sample RAFT Training Example (P=80% case, golden doc likely present) ---\")\n",
        "print(hf_dataset[0]['input'])\n",
        "print(f\"\\nEXPECTED OUTPUT:\\n{hf_dataset[0]['output']}\\n\")"
      ],
      "metadata": {
        "id": "UrjQlbHGChvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model and Tokenizer"
      ],
      "metadata": {
        "id": "m-k4UTuiC7HN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using 4 bit quantization to make the model smaller in memoery and LoRA(low rank adapation) to reduce the number of parameters that need to be trained"
      ],
      "metadata": {
        "id": "Fooi8Q4qY9Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoTokenizer automatically download and configure tokenzier for a given model\n"
      ],
      "metadata": {
        "id": "G-XUhrc7ZdZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load Model, Tokenizer, and Configure LoRA (Corrected Version)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Using a small, accessible Llama-like model for this demo\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading base model and tokenizer: {MODEL_NAME}\")\n",
        "\n",
        "\n",
        "# 1. Create the BitsAndBytesConfig object for 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True\n",
        ")\n",
        "#Download and loading the tokenizer that was trained with tinyLlama model\n",
        "#tokenizer responsible for converting text to numbers(tokens)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Set padding token\n",
        "\n",
        "# 2. Pass the new config object to the from_pretrained method\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\", # Automatically map model layers to available devices (GPU/CPU)\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config, # Pass the correct config object here\n",
        ")\n",
        "\n",
        "# Prepare model for LoRA training(low rank adaptation)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"\\nModel prepared for LoRA fine-tuning:\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "UoWOTkIbC9Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tokenize Dataset for Training"
      ],
      "metadata": {
        "id": "oIIJPY9VDAmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares the text based dataset for the model by converting the text into numbers(tokens) ,We create a single text string by concatenating the input and output text,Am .eos or end of sentence token is added at very end to signal that the sequence is compelete"
      ],
      "metadata": {
        "id": "AU88zOtRdqX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the CELL 2 i created my raft training dataset then this dataset is converted to standard format (dataset object) then the data is tokenized and prepared for the model creating a new dataset called tokenized dataset(done in cell 4) finally this tokenized dataset is passed to the trainer via train_dataset argument"
      ],
      "metadata": {
        "id": "hcGE9Fo-fYR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Tokenize the Dataset (Corrected Version)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Combine input and output for Causal LM training\n",
        "    full_text = [f\"{inp}{out}{tokenizer.eos_token}\" for inp, out in zip(examples[\"input\"], examples[\"output\"])]\n",
        "\n",
        "    # Tokenize the full text\n",
        "    tokenized_inputs = tokenizer(\n",
        "        full_text,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # --- THIS IS THE FIX ---\n",
        "    # Create a 'labels' column for the loss calculation by copying the input_ids\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "# Rerun the mapping with the updated function\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=[\"input\", \"output\"])\n",
        "print(\"Dataset tokenized.\")\n",
        "print(f\"Sample of tokenized data now includes a 'labels' key: {tokenized_dataset.column_names}\")"
      ],
      "metadata": {
        "id": "kzDac8ywDAMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Run the Fine-Tuning Job"
      ],
      "metadata": {
        "id": "NQwbM1Z_D68H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "output_dir is where all training outputs,like model checkpoints and logs will be saved.\n"
      ],
      "metadata": {
        "id": "Xc3HB0tFej5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Configure the training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    optim=\"paged_adamw_8bit\",      # Memory-efficient optimizer\n",
        "    fp16=True,                     # Enable mixed-precision training\n",
        "    report_to=\"none\",              # This is the line you need to add\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start the fine-tuning process\n",
        "print(\"\\nStarting local SFT training... ðŸš€\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# Save the resulting LoRA adapter\n",
        "trainer.save_model(\"./fine_tuned_raft_adapter\")\n",
        "print(\"Fine-tuned LoRA adapter saved to './fine_tuned_raft_adapter'\")"
      ],
      "metadata": {
        "id": "DcWeeu1VC_QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1-installs the required python libraries for dataprocessing and fine tuning\n",
        "\n",
        "Cell 2-Generates a custom ,RAFT style dataset of questions and answers to be used for training\n",
        "\n",
        "Cell 3-Loads the base language model,quantizes it to save the memory and prepares it for efficient LoRA fine tuning\n",
        "\n",
        "Cell 4-Tokenizes the custom dataset which i created using the ideology of RAFT,tokenization converts the text into numerical format that model can process\n",
        "\n",
        "Cell 5-Excecutes the model fine tuning process usign the preapred data and saves the resulting lightweight LoRA adapter"
      ],
      "metadata": {
        "id": "lxUdNynAhInA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- 1. Define Model and Adapter Paths ---\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "adapter_path = \"./fine_tuned_raft_adapter\"\n",
        "\n",
        "# --- 2. Load the Tokenizer and Quantized Base Model ---\n",
        "print(\"Loading base model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "# --- 3. Load the Fine-Tuned LoRA Adapter ---\n",
        "print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "model = model.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Fine-tuned model loaded successfully! âœ…\")\n",
        "\n",
        "\n",
        "# --- 4. Create a Test Prompt in the Correct Format ---\n",
        "test_question = \"Who painted the Mona Lisa?\"\n",
        "test_context = \"\"\"\n",
        "[Document 1: The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci.]\n",
        "[Document 2: It is considered an archetypal masterpiece of the Italian Renaissance.]\n",
        "\"\"\"\n",
        "instruction = \"Instruction: Given the question and context, provide a logical reasoning and the final answer. Use the format: ##Reason: {reason}\\n##Answer: {answer}.\"\n",
        "\n",
        "prompt = f\"Question: {test_question}\\nContext: {test_context}\\n{instruction}\"\n",
        "\n",
        "print(\"\\n--- Test Prompt ---\")\n",
        "print(prompt)\n",
        "\n",
        "\n",
        "# --- 5. Generate and Print the Response ---\n",
        "print(\"\\n--- Generating Response from Fine-Tuned Model ---\")\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode the output, skipping the prompt part to show only the new generation\n",
        "response_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
        "response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "bPQ7S6cED9PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code loads newly fine tuned LoRA adapter ,merges with base model and then use this combined model to answer a new question.\n",
        "\n",
        "It loads the original TinyLlama model,it then loads the small fine_tuned_raft_adapter ,The peft Model takes the model and merges with the small adapter weights on top of it\n",
        "\n",
        "Model.eval() switches model to evaluation mode\n",
        "\n",
        "When a user enters a prompt using the same format as of RAFT,the text prompt is converted by tokenizer into numerical token that model can understand"
      ],
      "metadata": {
        "id": "RPxo92hAiMoz"
      }
    }
  ]
}