{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYwSVZA4BVJD"
      },
      "source": [
        "Step 1-Cleaning the Raw document and preprocess the text(remove noise and normalize encoding) , Best approach is to remove extra whitespace using python's built in regular expression library(re)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYmT6Zz-9fRw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import textwrap\n",
        "\n",
        "# 1. Document Creation / Ingestion\n",
        "# We'll use the \"Dataset\" generated aritfically using an LLM which is suitable for graphrag.\n",
        "# It is stored here as a multiline string.\n",
        "\n",
        "doc_text = \"\"\"\n",
        "Whitepaper: AuraFlow - A Decentralized Framework for Adaptive AI\n",
        "\n",
        "1. Introduction\n",
        "\n",
        "In the landscape of artificial intelligence, traditional monolithic systems present significant challenges in scalability, adaptability, and resilience. AuraFlow is a novel, decentralized framework designed to overcome these limitations by enabling the creation of stateful, multi-agent AI systems. It is built on the core principles of decentralization, modularity, and emergent intelligence. By distributing tasks across specialized, independent agents, AuraFlow creates robust systems that can adapt to new information in real-time without requiring complete model retraining. This document outlines the core components, architecture, and primary use cases of the AuraFlow framework.\n",
        "\n",
        "---\n",
        "\n",
        "2. Core Components\n",
        "\n",
        "The AuraFlow framework is composed of four primary components that work in synergy. Each component is a specialized agent with a distinct role.\n",
        "\n",
        "2.1 The Cognition Core\n",
        "The Cognition Core is the central reasoning engine of an AuraFlow instance. Unlike traditional neural networks, it utilizes a proprietary Probabilistic Logic Network (PLN) for decision-making. This allows the Core to handle uncertainty, reason with incomplete information, and provide transparent, explainable outputs. The Cognition Core is responsible for high-level task decomposition, planning, and final response synthesis. Its performance is heavily dependent on the quality of processed data it receives from the Data Weavers.\n",
        "\n",
        "2.2 Data Weavers\n",
        "Data Weavers are specialized agents tasked with data ingestion, preprocessing, and normalization. Each Weaver can be configured to handle specific data modalities, such as unstructured text, images, streaming time-series data, or structured database records. They clean and transform raw data into a standardized format that the Cognition Core can efficiently process. This modular approach allows an AuraFlow system to seamlessly integrate new data sources by simply deploying a new, appropriately configured Data Weaver. Communication between Data Weavers and the Cognition Core is managed by the Synapse Bridge.\n",
        "\n",
        "2.3 The Synapse Bridge\n",
        "The Synapse Bridge is the high-bandwidth, low-latency communication backbone of the AuraFlow framework. It facilitates interaction between all components, primarily managing the flow of information from the Data Weavers to the Cognition Core and broadcasting the Core's directives to other agents. It uses a custom, lightweight data exchange protocol called the Neuro-Link Protocol, which ensures secure and efficient data transmission, a critical feature for real-time applications.\n",
        "\n",
        "2.4 The Sentinel Layer\n",
        "The Sentinel Layer acts as the ethical and security guardian of the system. It is a specialized validation agent that monitors the outputs and behavior of the Cognition Core in real-time. The Sentinel Layer is responsible for applying ethical constraints, enforcing operational boundaries, and preventing the generation of harmful or biased outputs. It can veto or flag a decision made by the Cognition Core if it violates pre-defined rules, ensuring that the system operates safely and responsibly.\n",
        "\n",
        "---\n",
        "\n",
        "3. System Architecture and Data Flow\n",
        "\n",
        "The architecture of AuraFlow is inherently decentralized. A typical workflow for processing a user query follows these steps:\n",
        "1.  A query is received by the system.\n",
        "2.  Relevant Data Weavers are activated to gather and process external or internal data related to the query.\n",
        "3.  The processed data is transmitted securely via the Synapse Bridge to the Cognition Core.\n",
        "4.  The Cognition Core uses its PLN to analyze the data, reason about the query, and formulate a plan or response.\n",
        "5.  Before being finalized, the proposed response is sent to the Sentinel Layer for validation.\n",
        "6.  If approved, the final response is generated and delivered.\n",
        "\n",
        "This modular data flow ensures that each component can be independently upgraded or scaled. For instance, if processing speed becomes a bottleneck, more Data Weaver instances can be deployed without altering the Cognition Core.\n",
        "\n",
        "---\n",
        "\n",
        "4. Key Applications\n",
        "\n",
        "The unique architecture of AuraFlow makes it suitable for complex, dynamic environments.\n",
        "\n",
        "Real-time Market Analysis: An AuraFlow system can deploy multiple Data Weavers to monitor financial news, social media sentiment, and stock market data simultaneously. The Cognition Core can then synthesize this information to identify trends and risks, while the Sentinel Layer ensures that trading recommendations adhere to regulatory compliance.\n",
        "\n",
        "Autonomous Scientific Research: In this scenario, a network of AuraFlow instances can collaborate on research. One instance could use its Data Weavers to analyze experimental data from lab equipment, while another analyzes existing scientific literature. The Cognition Cores could then exchange findings via their Synapse Bridges to formulate new hypotheses, accelerating the pace of discovery.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Clean and preprocess text\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"A simple function to clean up text data.\"\"\"\n",
        "    # Replace multiple newlines with a single one\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply the preprocessing function to our document\n",
        "cleaned_doc = preprocess_text(doc_text)\n",
        "\n",
        "# --- Verification (Optional) ---\n",
        "# Print the first 500 characters of the cleaned document to see the result.\n",
        "print(\"--- Cleaned Document (First 500 Characters) ---\")\n",
        "# textwrap.fill helps in pretty-printing the text to fit the screen width\n",
        "print(textwrap.fill(cleaned_doc[:500], width=80))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYJ-ZemXDBGf"
      },
      "source": [
        "Step 2-Chunking and Vectorization , We implement the chunking process using the spaCy NLP library which breaks the document into individual sentence , Then for vectorization we will use a sentence transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtPHLSMMCFmJ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install all required libraries\n",
        "!pip install -q spacy sentence-transformers\n",
        "\n",
        "# Step 2: Download the spaCy English model\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6BvQ79TOEOfj"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# The cleaned document from our first step.\n",
        "cleaned_doc = \"\"\"\n",
        "Whitepaper: AuraFlow - A Decentralized Framework for Adaptive AI\n",
        "1. Introduction\n",
        "In the landscape of artificial intelligence, traditional monolithic systems present significant challenges in scalability, adaptability, and resilience. AuraFlow is a novel, decentralized framework designed to overcome these limitations by enabling the creation of stateful, multi-agent AI systems. It is built on the core principles of decentralization, modularity, and emergent intelligence. By distributing tasks across specialized, independent agents, AuraFlow creates robust systems that can adapt to new information in real-time without requiring complete model retraining. This document outlines the core components, architecture, and primary use cases of the AuraFlow framework.\n",
        "2. Core Components\n",
        "The AuraFlow framework is composed of four primary components that work in synergy. Each component is a specialized agent with a distinct role.\n",
        "2.1 The Cognition Core\n",
        "The Cognition Core is the central reasoning engine of an AuraFlow instance. Unlike traditional neural networks, it utilizes a proprietary Probabilistic Logic Network (PLN) for decision-making. This allows the Core to handle uncertainty, reason with incomplete information, and provide transparent, explainable outputs. The Cognition Core is responsible for high-level task decomposition, planning, and final response synthesis. Its performance is heavily dependent on the quality of processed data it receives from the Data Weavers.\n",
        "2.2 Data Weavers\n",
        "Data Weavers are specialized agents tasked with data ingestion, preprocessing, and normalization. Each Weaver can be configured to handle specific data modalities, such as unstructured text, images, streaming time-series data, or structured database records. They clean and transform raw data into a standardized format that the Cognition Core can efficiently process. This modular approach allows an AuraFlow system to seamlessly integrate new data sources by simply deploying a new, appropriately configured Data Weaver. Communication between Data Weavers and the Cognition Core is managed by the Synapse Bridge.\n",
        "2.3 The Synapse Bridge\n",
        "The Synapse Bridge is the high-bandwidth, low-latency communication backbone of the AuraFlow framework. It facilitates interaction between all components, primarily managing the flow of information from the Data Weavers to the Cognition Core and broadcasting the Core's directives to other agents. It uses a custom, lightweight data exchange protocol called the Neuro-Link Protocol, which ensures secure and efficient data transmission, a critical feature for real-time applications.\n",
        "2.4 The Sentinel Layer\n",
        "The Sentinel Layer acts as the ethical and security guardian of the system. It is a specialized validation agent that monitors the outputs and behavior of the Cognition Core in real-time. The Sentinel Layer is responsible for applying ethical constraints, enforcing operational boundaries, and preventing the generation of harmful or biased outputs. It can veto or flag a decision made by the Cognition Core if it violates pre-defined rules, ensuring that the system operates safely and responsibly.\n",
        "3. System Architecture and Data Flow\n",
        "The architecture of AuraFlow is inherently decentralized. A typical workflow for processing a user query follows these steps:\n",
        "1. A query is received by the system.\n",
        "2. Relevant Data Weavers are activated to gather and process external or internal data related to the query.\n",
        "3. The processed data is transmitted securely via the Synapse Bridge to the Cognition Core.\n",
        "4. The Cognition Core uses its PLN to analyze the data, reason about the query, and formulate a plan or response.\n",
        "5. Before being finalized, the proposed response is sent to the Sentinel Layer for validation.\n",
        "6. If approved, the final response is generated and delivered.\n",
        "This modular data flow ensures that each component can be independently upgraded or scaled. For instance, if processing speed becomes a bottleneck, more Data Weaver instances can be deployed without altering the Cognition Core.\n",
        "4. Key Applications\n",
        "The unique architecture of AuraFlow makes it suitable for complex, dynamic environments.\n",
        "Real-time Market Analysis: An AuraFlow system can deploy multiple Data Weavers to monitor financial news, social media sentiment, and stock market data simultaneously. The Cognition Core can then synthesize this information to identify trends and risks, while the Sentinel Layer ensures that trading recommendations adhere to regulatory compliance.\n",
        "Autonomous Scientific Research: In this scenario, a network of AuraFlow instances can collaborate on research. One instance could use its Data Weavers to analyze experimental data from lab equipment, while another analyzes existing scientific literature. The Cognition Cores could then exchange findings via their Synapse Bridges to formulate new hypotheses, accelerating the pace of discovery.\n",
        "\"\"\"\n",
        "\n",
        "# --- 1. Chunking with spaCy ---\n",
        "print(\"---  Loading spaCy model and chunking document... ---\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(cleaned_doc)\n",
        "sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "def create_sentence_chunks(sentences, chunk_size=500, overlap=50):\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) > chunk_size and current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "            overlap_text = ' '.join(current_chunk.split()[-overlap:])\n",
        "            current_chunk = overlap_text + \" \" + sentence\n",
        "        else:\n",
        "            current_chunk += \" \" + sentence\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "chunks = create_sentence_chunks(sentences, chunk_size=500, overlap=30)\n",
        "print(f\"---  Document chunked into {len(chunks)} parts. ---\")\n",
        "print(\"\\nExample Chunk (Chunk 0):\")\n",
        "print(chunks[0])\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- 2. Vectorization (Embedding) ---\n",
        "print(\"\\n---  Loading embedding model and generating embeddings... ---\")\n",
        "# The first time this line runs, it will download the model. Please be patient.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device='cpu')\n",
        "embeddings = model.encode(chunks)\n",
        "\n",
        "print(f\"---  Embeddings Generated ---\")\n",
        "print(f\"Shape of embeddings matrix: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMEbSd6wF0db"
      },
      "source": [
        "3)Now storing the Created Embeddings in a database-chroma DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_lmIzTKdFd9g"
      },
      "outputs": [],
      "source": [
        "# Install the library for ChromaDB\n",
        "!pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nWRaZeZKF71f"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "# --- 1. Initialize ChromaDB Client ---\n",
        "client = chromadb.Client()\n",
        "\n",
        "# --- 2. Create a Collection ---\n",
        "# A collection is where you'll store your embeddings, documents, and metadata.\n",
        "collection = client.get_or_create_collection(name=\"auraflow_docs\")\n",
        "\n",
        "# --- 3. Prepare Data and Add to Collection ---\n",
        "# ChromaDB requires a unique ID for each entry. We can simply use the index\n",
        "# of each chunk as a string.\n",
        "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "\n",
        "# Add the data to the collection.\n",
        "# This single command uploads your chunks, their embeddings, and their IDs.\n",
        "collection.add(\n",
        "    embeddings=embeddings,\n",
        "    documents=chunks,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "# --- Verification (Optional) ---\n",
        "# Check how many items are in the collection.\n",
        "count = collection.count()\n",
        "print(f\"---  Collection '{collection.name}' created successfully. ---\")\n",
        "print(f\"---  It contains {count} documents. ---\")\n",
        "\n",
        "# You can also peek at the first few items to see what they look like.\n",
        "print(\"\\n---  Peek at the first 2 items in the collection: ---\")\n",
        "peek_result = collection.peek(limit=2)\n",
        "print(peek_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-y6NsUnHNxF"
      },
      "source": [
        "Step 4- Building the graph structure for Graph Rag-\n",
        "\n",
        "Plan-\n",
        "1) Each text chunk we created will become a node in our graph\n",
        "2)Edges are relationships- We will connect these nodes based on two simple rules-\n",
        "   a)Sequential Links-We will connect each chunk to chunk that comes immediately after it in the document\n",
        "   b)Similarity links-for a given chunk we ffind other chunk that are sementically very similar we will connect them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-rEgNIEGQyW"
      },
      "outputs": [],
      "source": [
        "# Install scikit-learn for similarity calculations\n",
        "!pip install -q scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcLEg6iFIiyM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- 1. Initialize the Graph ---\n",
        "# We'll use a dictionary where keys are chunk indices (nodes) and\n",
        "# values are lists of connected chunk indices (edges).\n",
        "graph = {i: [] for i in range(len(chunks))}\n",
        "\n",
        "\n",
        "# --- 2. Add Sequential Edges ---\n",
        "#  Connect each chunk to the one that follows it.\n",
        "print(\"---  Adding sequential edges... ---\")\n",
        "for i in range(len(chunks) - 1):\n",
        "    graph[i].append(i + 1)\n",
        "    # Optional: If you want a two-way connection (undirected graph)\n",
        "    # graph[i+1].append(i)\n",
        "\n",
        "print(\"Sequential edges added.\")\n",
        "\n",
        "\n",
        "# --- 3. Add Similarity Edges ---\n",
        "#  Connect chunks that are semantically similar.\n",
        "print(\"\\n---  Calculating similarities and adding similarity edges... ---\")\n",
        "\n",
        "# First, calculate the cosine similarity matrix for all embeddings\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Define a threshold for what we consider \"similar\"\n",
        "# This is a key parameter to tune. A higher threshold means fewer, more relevant links.\n",
        "SIMILARITY_THRESHOLD = 0.80\n",
        "\n",
        "# Iterate through the matrix to find pairs of chunks above the threshold\n",
        "# We use np.where to find the indices efficiently\n",
        "similar_pairs = np.where(similarity_matrix > SIMILARITY_THRESHOLD)\n",
        "\n",
        "for i, j in zip(*similar_pairs):\n",
        "    if i != j:  # Don't connect a chunk to itself\n",
        "        # Add an edge between the two similar chunks\n",
        "        if j not in graph[i]:\n",
        "            graph[i].append(j)\n",
        "        # Optional: For an undirected graph\n",
        "        # if i not in graph[j]:\n",
        "        #     graph[j].append(i)\n",
        "\n",
        "print(f\"Similarity edges added with threshold > {SIMILARITY_THRESHOLD}.\")\n",
        "\n",
        "\n",
        "# --- 4. Verification (Optional) ---\n",
        "# Let's inspect the connections for a specific chunk (e.g., the first one).\n",
        "chunk_id_to_inspect = 0\n",
        "connected_nodes = graph[chunk_id_to_inspect]\n",
        "\n",
        "print(f\"\\n---  Graph built successfully. ---\")\n",
        "print(f\"\\n---  Inspecting connections for Chunk {chunk_id_to_inspect}: ---\")\n",
        "print(\"Original Chunk Text:\")\n",
        "print(chunks[chunk_id_to_inspect])\n",
        "print(\"\\nIt is connected to the following chunks:\")\n",
        "for node_id in connected_nodes:\n",
        "    connection_type = \"Sequential\" if node_id == chunk_id_to_inspect + 1 else \"Similarity\"\n",
        "    print(f\"  - Chunk {node_id} ({connection_type})\")\n",
        "    # print(f\"    Text: {chunks[node_id][:100]}...\") # Uncomment to see text snippet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhYLG5pUJfbD"
      },
      "source": [
        "5th Step-User query and initial retrieval of the query based on vector databse only ,we will embed the query and then ask the database and ask ChromaDb to return top 3 most similar results to our query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWCnYReLJF1F"
      },
      "outputs": [],
      "source": [
        "def query_vector_db_with_scores(query: str, collection, model, n_results=3):\n",
        "    \"\"\"\n",
        "    Takes a user query, embeds it, and retrieves the most similar chunks\n",
        "    and their scores from the ChromaDB collection.\n",
        "    \"\"\"\n",
        "    # 1. Embed the user query\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "\n",
        "    # 2. Query the collection\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n_results\n",
        "    )\n",
        "\n",
        "    # 3. Extract the document chunks and their distances\n",
        "    retrieved_chunks = results['documents'][0]\n",
        "    distances = results['distances'][0]\n",
        "\n",
        "    return retrieved_chunks, distances\n",
        "\n",
        "# --- Let's Test the Function ---\n",
        "\n",
        "query = \"How does AuraFlow handle security and ethics?\"\n",
        "print(f\"---  Query: {query} ---\")\n",
        "\n",
        "# Get both the chunks and their scores\n",
        "initial_chunks, distances = query_vector_db_with_scores(query, collection, model)\n",
        "\n",
        "# Display the results with scores\n",
        "print(\"\\n---  Top 3 initial results from Vector DB: ---\")\n",
        "for i, (chunk, dist) in enumerate(zip(initial_chunks, distances)):\n",
        "    # Calculate a more intuitive confidence score (1.0 is a perfect match)\n",
        "    confidence = 1 - dist\n",
        "\n",
        "    print(f\"\\nResult {i+1} (Confidence: {confidence:.2%}, Distance: {dist:.4f}):\")\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8oSz4aNJ-EB"
      },
      "source": [
        "6)From the retrieved top k chunks we will find the neighbours of the graph structure created (basically)explore the graph structue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO_VdFg_JzBB"
      },
      "outputs": [],
      "source": [
        "def expand_with_graph(initial_chunks, graph, all_chunks):\n",
        "    \"\"\"\n",
        "    Expands the initial list of chunks by traversing the graph to find neighbors.\n",
        "    \"\"\"\n",
        "    # Use a set to automatically handle duplicates\n",
        "    expanded_chunks_set = set(initial_chunks)\n",
        "\n",
        "    # Create a quick lookup map from chunk text to its index\n",
        "    chunk_to_id = {chunk: i for i, chunk in enumerate(all_chunks)}\n",
        "\n",
        "    # Iterate through the initial chunks to find their neighbors\n",
        "    for chunk_text in initial_chunks:\n",
        "        # 1. Find the node ID (index) of the current chunk\n",
        "        node_id = chunk_to_id.get(chunk_text)\n",
        "        if node_id is None:\n",
        "            continue # Should not happen if all_chunks is correct\n",
        "\n",
        "        # 2. Get all neighbors of this node from the graph\n",
        "        neighbor_ids = graph.get(node_id, [])\n",
        "\n",
        "        # 3. Add the text of each neighbor to our set\n",
        "        for neighbor_id in neighbor_ids:\n",
        "            neighbor_text = all_chunks[neighbor_id]\n",
        "            expanded_chunks_set.add(neighbor_text)\n",
        "\n",
        "    return list(expanded_chunks_set)\n",
        "\n",
        "# --- Let's Test the Function ---\n",
        "\n",
        "# We'll use the 'initial_chunks' retrieved from the last step.\n",
        "# If you don't have it, run this line to generate it for a query:\n",
        "# initial_chunks, _ = query_vector_db_with_scores(\"What is the Cognition Core?\", collection, model)\n",
        "\n",
        "print(f\"---  Number of initial chunks: {len(initial_chunks)} ---\")\n",
        "print(\"Initial Chunks:\", initial_chunks)\n",
        "\n",
        "# Run the expansion\n",
        "graph_expanded_chunks = expand_with_graph(initial_chunks, graph, chunks)\n",
        "\n",
        "print(f\"\\n---  Number of chunks after graph expansion: {len(graph_expanded_chunks)} ---\")\n",
        "print(\"\\n---  Expanded Chunks (Initial + Neighbors): ---\")\n",
        "for i, chunk in enumerate(graph_expanded_chunks):\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58xstY8TLGWB"
      },
      "source": [
        "7)Combining the Extracted information from both of them and using it and finding top queiries suitable for generation (which are to be given to LLM) , Here we will take the combined initial +graph queries and calculate the direct similarity of each one of them with the  original query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpwKrp-QLvg4"
      },
      "source": [
        "\n",
        "In this we have created a re rank function to rank the similarity with the initial queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG5qKV5PLBrD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def rerank_chunks(query: str, chunks_to_rank: list, model, top_n=5):\n",
        "    \"\"\"\n",
        "    Reranks a list of chunks based on their semantic similarity to a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's original query.\n",
        "        chunks_to_rank (list): The combined list of chunks to be reranked.\n",
        "        model: The SentenceTransformer embedding model.\n",
        "        top_n (int): The number of top chunks to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A sorted list of the top_n most relevant chunk texts.\n",
        "    \"\"\"\n",
        "    # 1. Embed the query and the chunks\n",
        "    query_embedding = model.encode(query)\n",
        "    chunk_embeddings = model.encode(chunks_to_rank)\n",
        "\n",
        "    # 2. Calculate cosine similarity between the query and all chunks\n",
        "    # Reshape query embedding to be a 2D array for the function\n",
        "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
        "\n",
        "    # 3. Pair each chunk with its similarity score\n",
        "    scored_chunks = list(zip(chunks_to_rank, similarities))\n",
        "\n",
        "    # 4. Sort the chunks by score in descending order\n",
        "    scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # 5. Return the text of the top_n chunks\n",
        "    top_chunks = [chunk for chunk, score in scored_chunks[:top_n]]\n",
        "\n",
        "    return top_chunks\n",
        "\n",
        "# --- Let's Test the Function ---\n",
        "\n",
        "# Assume 'graph_expanded_chunks' from the previous step is available.\n",
        "# Let's also define the query we used to get these chunks.\n",
        "query = \"What is the Cognition Core and how does it work?\"\n",
        "\n",
        "# For demonstration, let's create 'graph_expanded_chunks' if it's not in memory\n",
        "if 'graph_expanded_chunks' not in locals():\n",
        "    initial_chunks, _ = query_vector_db_with_scores(query, collection, model, n_results=3)\n",
        "    graph_expanded_chunks = expand_with_graph(initial_chunks, graph, chunks)\n",
        "\n",
        "print(f\"---  Reranking {len(graph_expanded_chunks)} combined chunks... ---\")\n",
        "\n",
        "# Run the reranking\n",
        "reranked_chunks = rerank_chunks(query, graph_expanded_chunks, model, top_n=5)\n",
        "\n",
        "# Display the final, most relevant chunks\n",
        "print(f\"\\n---  Top {len(reranked_chunks)} Reranked & Filtered Chunks: ---\")\n",
        "for i, chunk in enumerate(reranked_chunks):\n",
        "    print(f\"\\nRank {i+1}:\")\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkBzqkTbL2yg"
      },
      "source": [
        "8)Concatenate the top ranked chunks into the context window which is to be given to LLM for generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHLyWYIELo5Z"
      },
      "outputs": [],
      "source": [
        "def merge_chunks(chunks: list) -> str:\n",
        "    \"\"\"\n",
        "    Concatenates a list of text chunks into a single string,\n",
        "    separated by double newlines.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): A list of strings (the reranked chunks).\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing the merged context.\n",
        "    \"\"\"\n",
        "    return \"\\n\\n---\\n\\n\".join(chunks)\n",
        "\n",
        "if 'reranked_chunks' not in locals():\n",
        "    # If you ran the previous step, this list will be populated with actual data.\n",
        "    reranked_chunks = [\n",
        "        \"The Cognition Core is the central reasoning engine of an AuraFlow instance. Unlike traditional neural networks, it utilizes a proprietary Probabilistic Logic Network (PLN) for decision-making.\",\n",
        "        \"The Cognition Core is responsible for high-level task decomposition, planning, and final response synthesis. Its performance is heavily dependent on the quality of processed data it receives from the Data Weavers.\",\n",
        "        \"The Cognition Core uses its PLN to analyze the data, reason about the query, and formulate a plan or response.\"\n",
        "    ]\n",
        "\n",
        "# Merge the reranked chunks into the final context window\n",
        "final_context = merge_chunks(reranked_chunks)\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"---  Final Merged Context Ready for LLM ---\")\n",
        "print(final_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVxoVFuNolV"
      },
      "source": [
        "9)Using LLM for final query output generation-\n",
        " I am using Gemini for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia075KAHOF86"
      },
      "source": [
        "API KEY SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK5U5fCLMLWV"
      },
      "outputs": [],
      "source": [
        "# Install the library for Google's Generative AI\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "\n",
        "# Configure the API key from Colab secrets\n",
        "try:\n",
        "    api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "    print(\" API Key configured successfully!\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(' ERROR: Secret \"GOOGLE_API_KEY\" not found. Please follow Step 2 to set it up.')\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjW8PNBGPvu0"
      },
      "source": [
        "Final generation using LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li0DpdM3Ob_o"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import textwrap\n",
        "\n",
        "# This code assumes your API key is already configured.\n",
        "\n",
        "def generate_llm_answer(query: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a final answer using the Gemini LLM based on the query and context.\n",
        "    \"\"\"\n",
        "    prompt_template = f\"\"\"\n",
        "    Answer the following query based only on the provided context.\n",
        "    If the context does not contain the answer, state that the information is not available in the document.\n",
        "    Be concise and do not add any information that is not present in the context.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUERY:\n",
        "    {query}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "        response = model.generate_content(prompt_template)\n",
        "\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while generating the answer: {e}\"\n",
        "\n",
        "# --- EXAMPLE EXECUTION ---\n",
        "query = \"What is the Cognition Core and how does it work?\"\n",
        "\n",
        "if 'final_context' not in locals():\n",
        "    final_context = \"The Cognition Core is the central reasoning engine of an AuraFlow instance. It utilizes a Probabilistic Logic Network (PLN) for decision-making, allowing it to handle uncertainty. It is responsible for task decomposition, planning, and response synthesis.\"\n",
        "\n",
        "answer = generate_llm_answer(query, final_context)\n",
        "\n",
        "# ---  FINAL, FORMATTED OUTPUT ---\n",
        "\n",
        "# 1. Print the user query\n",
        "print(\"---  USER QUERY ---\")\n",
        "print(query)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# 2. Print the context that was sent to the LLM (with text wrap)\n",
        "print(\"---  CONTEXT PROVIDED TO LLM ---\")\n",
        "print(textwrap.fill(final_context, width=80))\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# 3. Print the final answer from the LLM (with text wrap)\n",
        "print(\"---  FINAL LLM-GENERATED ANSWER ---\")\n",
        "print(textwrap.fill(answer, width=80))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p58gv-lPdeF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
